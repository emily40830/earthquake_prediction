options(stringsAsFactors = F)
library(e1071)
library(tidyverse)
library(stringer)
library(e1071)
library(tidyverse)
library(stringr)
options(stringsAsFactors = F)
library(dplyr)
tsmip<-read.csv('TSMIP_distinguish.csv',header = F)
View(tsmip)
ASUS<-read.csv('ASUS.csv',header = F)
View(tsmip)
names<-c("ZC","IQR","CAV")
names(ASUS)<-names
names(tsmip)<-names
tsmip_predict<-read.csv('TSMIP_predict.csv',header = F)
View(tsmip_predict)
help(min)
library(stats)
ASUS$ZC<-ASUS%>%
mutate(ZC=ZC-min(ZC)/max(ZC)-min(ZC))
View(ASUS)
ASUS<-read.csv('ASUS.csv',header = F)
names(ASUS)<-names
ASUS<-ASUS%>%
mutate(NZC=ZC-min(ZC)/max(ZC)-min(ZC))
min(ASUS$ZC)
ASUS<-ASUS%>%
mutate(NZC=(ZC-min(ZC))/(max(ZC)-min(ZC)))
View(ASUS)
ASUS<-read.csv('ASUS.csv',header = F)
names(ASUS)<-names
ASUS<-ASUS%>%
mutate(ZC=(ZC-min(ZC))/(max(ZC)-min(ZC)))
View(ASUS)
ASUS<-ASUS%>%
mutate(ZC=(ZC-min(ZC))/(max(ZC)-min(ZC)))%>%
mutate(IQR=(IQR-min(IQR))/(max(IQR)-min(IQR)))%>%
mutate(CAV=(CAV-min(CAV))/(max(CAV)-min(CAV)))
tsmip<-tsmip%>%
mutate(ZC=(ZC-min(ZC))/(max(ZC)-min(ZC)))%>%
mutate(IQR=(IQR-min(IQR))/(max(IQR)-min(IQR)))%>%
mutate(CAV=(CAV-min(CAV))/(max(CAV)-min(CAV)))
View(tsmip)
tsmip<-read.csv('TSMIP_distinguish.csv',header = F)
ASUS<-read.csv('ASUS.csv',header = F)
tsmip_predict<-read.csv('TSMIP_predict.csv',header = F)
names<-c("ZC","IQR","CAV")
names(ASUS)<-names
names(tsmip)<-names
#normalize
ASUS<-ASUS%>%
mutate(ZC=(ZC-min(ZC))/(max(ZC)-min(ZC)))%>%
mutate(IQR=(IQR-min(IQR))/(max(IQR)-min(IQR)))%>%
mutate(CAV=(CAV-min(CAV))/(max(CAV)-min(CAV)))%>%
mutate(labels=1)
tsmip<-tsmip%>%
mutate(ZC=(ZC-min(ZC))/(max(ZC)-min(ZC)))%>%
mutate(IQR=(IQR-min(IQR))/(max(IQR)-min(IQR)))%>%
mutate(CAV=(CAV-min(CAV))/(max(CAV)-min(CAV)))%>%
mutate(labels=1)
train_data<-bind_rows(tsmip,ASUS)
View(train_data)
# 隨機構建訓練集與測試集
# 70%資料為訓練集，其餘為測試集
set.seed(2018)
samples <- sample(1:nrow(feature_tsne),
size = round(nrow(feature_tsne)*0.7))
samples <- sample(1:nrow(train_data),
size = round(nrow(train_data)*0.7))
trainset <- feature_tsne %>% slice(samples)
testset <- feature_tsne[-samples,]
trainset <- train_data %>% slice(samples)
testset <- train_data[-samples,]
# 建立 SVM 分類器model，機器學習主體函式
# labels~ 表示除去labels的資料，其他數據均進入機器學習中。 labels之資料作為分類標的。
# kernel 表示svm之核函式，此次選用Radial
model <- svm(labels~ ., data = trainset, kernel="radial")
View(testset)
# 隨機構建訓練集與測試集
# 70%資料為訓練集，其餘為測試集
set.seed(2018)
asus_samples<-sample(1:nrow(ASUS),size = round(nrow(ASUS)*0.7))
tsmip_samples<-sample(1:nrow(tsmip),size = round(nrow(tsmip)*0.7))
asus_train_set<-ASUS%>%slice(asus_samples)
tsmip_train_set<-tsmip%>%slice(tsmip_samples)
asus_test_set<-ASUS[-asus_samples]
tsmip_test_set<-tsmip[-tsmup_samples]
tsmip_test_set<-tsmip[-tsmip_samples]
trainset <- bind_rows(asus_train_set,tsmip_train_set)
testset <- bind_rows(asus_test_set,tsmip_test_set)
View(trainset)
View(testset)
View(trainset)
tsmip_test_set<-tsmip[-tsmip_samples]
View(asus_test_set)
# 隨機構建訓練集與測試集
# 70%資料為訓練集，其餘為測試集
set.seed(2018)
asus_samples<-sample(1:nrow(ASUS),size = round(nrow(ASUS)*0.7))
tsmip_samples<-sample(1:nrow(tsmip),size = round(nrow(tsmip)*0.7))
asus_train_set<-ASUS%>%slice(asus_samples)
tsmip_train_set<-tsmip%>%slice(tsmip_samples)
asus_test_set<-ASUS[-asus_samples,]
tsmip_test_set<-tsmip[-tsmip_samples,]
trainset <- bind_rows(asus_train_set,tsmip_train_set)
testset <- bind_rows(asus_test_set,tsmip_test_set)
# 建立 SVM 分類器model，機器學習主體函式
# labels~ 表示除去labels的資料，其他數據均進入機器學習中。 labels之資料作為分類標的。
# kernel 表示svm之核函式，此次選用Radial
model <- svm(labels~ ., data = trainset, kernel="radial")
tsmip<-read.csv('TSMIP_distinguish.csv',header = F)
ASUS<-read.csv('ASUS.csv',header = F)
tsmip_predict<-read.csv('TSMIP_predict.csv',header = F)
names<-c("ZC","IQR","CAV")
names(ASUS)<-names
names(tsmip)<-names
#normalize
ASUS<-ASUS%>%
mutate(ZC=(ZC-min(ZC))/(max(ZC)-min(ZC)))%>%
mutate(IQR=(IQR-min(IQR))/(max(IQR)-min(IQR)))%>%
mutate(CAV=(CAV-min(CAV))/(max(CAV)-min(CAV)))%>%
mutate(labels=0)
tsmip<-tsmip%>%
mutate(ZC=(ZC-min(ZC))/(max(ZC)-min(ZC)))%>%
mutate(IQR=(IQR-min(IQR))/(max(IQR)-min(IQR)))%>%
mutate(CAV=(CAV-min(CAV))/(max(CAV)-min(CAV)))%>%
mutate(labels=1)
train_data<-bind_rows(tsmip,ASUS)
# 隨機構建訓練集與測試集
# 70%資料為訓練集，其餘為測試集
set.seed(2018)
asus_samples<-sample(1:nrow(ASUS),size = round(nrow(ASUS)*0.7))
tsmip_samples<-sample(1:nrow(tsmip),size = round(nrow(tsmip)*0.7))
asus_train_set<-ASUS%>%slice(asus_samples)
tsmip_train_set<-tsmip%>%slice(tsmip_samples)
asus_test_set<-ASUS[-asus_samples,]
tsmip_test_set<-tsmip[-tsmip_samples,]
#samples <- sample(1:nrow(train_data),
#                 size = round(nrow(train_data)*0.7))
trainset <- bind_rows(asus_train_set,tsmip_train_set)
testset <- bind_rows(asus_test_set,tsmip_test_set)
# 建立 SVM 分類器model，機器學習主體函式
# labels~ 表示除去labels的資料，其他數據均進入機器學習中。 labels之資料作為分類標的。
# kernel 表示svm之核函式，此次選用Radial
model <- svm(labels~ ., data = trainset, kernel="radial")
# 預測函式主體
predicting  <- predict(model, testset[, -ncol(testset)])
# 對比統計預測與實際之差
table(predicting, testset$labels)
# 計算預測準確率
pre <- predicting == testset$labels
percent1 <- length(pre[pre == T]) / length(pre)
percent1
# 對比統計預測與實際之差
table(predicting, testset$labels)
# 計算預測準確率
pre <- predicting == testset$labels
percent1 <- length(pre[pre == T]) / length(pre)
percent1
View(trainset)
install.packages("plotly")
install.packages("plotly")
library(plotly)
library(e1071)
library(tidyverse)
library(stringr)
library(dplyr)
library(stats)
library(plotly)
options(stringsAsFactors = F)
p <- plot_ly(train_data, x = ~ZC, y = ~IQR, z = ~CAV, color = ~labels, colors = c('#BF382A', '#0C4B8E')) %>%
add_markers() %>%
layout(scene = list(xaxis = list(title = 'ZC'),
yaxis = list(title = 'IQR'),
zaxis = list(title = 'CAV')))
p
train_data$labels<-as.factor(train_data$labels)
p <- plot_ly(train_data, x = ~ZC, y = ~IQR, z = ~CAV, color = ~labels, colors = c('#BF382A', '#0C4B8E'),size=5) %>%
add_markers() %>%
layout(scene = list(xaxis = list(title = 'ZC'),
yaxis = list(title = 'IQR'),
zaxis = list(title = 'CAV')))
p
head(train_data)
View(ASUS)
View(tsmip)
View(tsmip)
library(e1071)
library(tidyverse)
library(stringr)
library(dplyr)
library(stats)
library(plotly)
options(stringsAsFactors = F)
tsmip<-read.csv('TSMIP_distinguish.csv',header = F)
ASUS<-read.csv('ASUS.csv',header = F)
View(tsmip)
View(ASUS)
View(tsmip)
tsmip_predict<-read.csv('TSMIP_predict.csv',header = F)
View(tsmip_predict)
names<-c("ZC","IQR","CAV")
names(ASUS)<-names
names(tsmip)<-names
ASUS<-ASUS%>%
#mutate(ZC=(ZC-min(ZC))/(max(ZC)-min(ZC)))%>%
#mutate(IQR=(IQR-min(IQR))/(max(IQR)-min(IQR)))%>%
#mutate(CAV=(CAV-min(CAV))/(max(CAV)-min(CAV)))%>%
mutate(labels=0)
tsmip<-tsmip%>%
#mutate(ZC=(ZC-min(ZC))/(max(ZC)-min(ZC)))%>%
#mutate(IQR=(IQR-min(IQR))/(max(IQR)-min(IQR)))%>%
#mutate(CAV=(CAV-min(CAV))/(max(CAV)-min(CAV)))%>%
mutate(labels=1)
# 隨機構建訓練集與測試集
# 70%資料為訓練集，其餘為測試集
set.seed(2018)
all<-ASUS%>%
bind_rows(tsmip)
all_samples<-sample(1:nrow(all),size = round(nrow(all)*0.7))
View(all)
all_train<-all%>%slice(all_samples)
View(all_train)
all_test<-all[-all_samples,]
# 建立 SVM 分類器model，機器學習主體函式
# labels~ 表示除去labels的資料，其他數據均進入機器學習中。 labels之資料作為分類標的。
# kernel 表示svm之核函式，此次選用Radial
model <- svm(labels~ ., data = all_train, kernel="radial")
# 預測函式主體
predicting  <- predict(model, all_test[, -ncol(all_test)])
# 對比統計預測與實際之差
table(predicting, testset$labels)
# 計算預測準確率
pre <- predicting == all_test$labels
percent1 <- length(pre[pre == T]) / length(pre)
percent1
# 對比統計預測與實際之差
table(predicting, testset$labels)
# 對比統計預測與實際之差
table(predicting, all_test$labels)
